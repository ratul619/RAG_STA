{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratul619/RAG_STA/blob/main/RAG_STA_trial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oNEVRWKRAYW5"
      },
      "outputs": [],
      "source": [
        "pip install -q chromadb sentence-transformers transformers accelerate safetensors torch --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ue1pWtVIA6Af"
      },
      "outputs": [],
      "source": [
        "pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "dc_0OqRYBWh7",
        "outputId": "c4fdc297-bad7-43e1-a986-cf4936d6eea3"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2550816532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from sentence_transformers.backend import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/backend/load.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_save_pretrained_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_should_export\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_warn_to_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0m_import_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0mimport_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_import_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mdefine_import_structure\u001b[0;34m(module_path, prefix)\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwill\u001b[0m \u001b[0madd\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0mto\u001b[0m \u001b[0mall\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \"\"\"\n\u001b[0;32m-> 2851\u001b[0;31m     \u001b[0mimport_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m     \u001b[0mspread_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspread_import_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2562\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__pycache__\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m             \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2560\u001b[0m     \u001b[0madjacent_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__pycache__\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "class LocalTimingRAG:\n",
        "    def __init__(self, model_name: Optional[str] = None, db_path: str = \"./timing_db\"):\n",
        "        # Embeddings (local, CPU friendly)\n",
        "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "        # Local LLM and tokenizer\n",
        "        self.llm_model, self.tokenizer = self._load_local_llm(model_name)\n",
        "\n",
        "        # Vector DB (local persistent)\n",
        "        self.chroma_client = chromadb.PersistentClient(path=db_path, settings=Settings(allow_reset=True))\n",
        "        self.collection = self.chroma_client.get_or_create_collection(\"timing_paths\")\n",
        "\n",
        "    def _load_local_llm(self, model_name: Optional[str] = None):\n",
        "        \"\"\"Load a local LLM with better instruction following\"\"\"\n",
        "        name = model_name or \"microsoft/DialoGPT-large\"  # Better than medium\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(name, use_fast=False)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        kwargs = {}\n",
        "        if torch.cuda.is_available():\n",
        "            kwargs[\"torch_dtype\"] = torch.float16\n",
        "            kwargs[\"device_map\"] = \"auto\"\n",
        "        else:\n",
        "            kwargs[\"torch_dtype\"] = torch.float32\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(name, **kwargs)\n",
        "        return model, tokenizer\n",
        "\n",
        "    def _read_json_safe(self, filepath: str):\n",
        "        \"\"\"Safely read JSON with multiple encoding attempts\"\"\"\n",
        "        encodings = ['utf-8', 'utf-16', 'utf-16le', 'utf-16be', 'latin-1', 'cp1252']\n",
        "\n",
        "        for encoding in encodings:\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding=encoding) as f:\n",
        "                    return json.load(f)\n",
        "            except (UnicodeDecodeError, json.JSONDecodeError):\n",
        "                continue\n",
        "\n",
        "        raise ValueError(f\"Could not read {filepath} with any supported encoding\")\n",
        "\n",
        "    def verify_timing_json(self, filepath: str):\n",
        "        \"\"\"Verify timing JSON structure with encoding detection\"\"\"\n",
        "        print(f\"\\n=== Verifying JSON: {filepath} ===\")\n",
        "        try:\n",
        "            data = self._read_json_safe(filepath)\n",
        "\n",
        "            print(f\"File: {filepath}\")\n",
        "            print(f\"Paths: {len(data.get('paths', []))}\")\n",
        "\n",
        "            for i, path in enumerate(data.get('paths', [])[:2]):\n",
        "                print(f\"  Path {i+1}:\")\n",
        "                print(f\"    Startpoint: {path.get('startpoint', {}).get('instance', 'N/A')}\")\n",
        "                print(f\"    Endpoint: {path.get('endpoint', {}).get('instance', 'N/A')}\")\n",
        "                print(f\"    Slack: {path.get('summary', {}).get('slack', 'N/A')}\")\n",
        "                print(f\"    Clock Group: {path.get('report', {}).get('group', 'N/A')}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filepath}: {e}\")\n",
        "\n",
        "    def index_timing_reports(self, reports_dir: str):\n",
        "        \"\"\"Index all timing JSON files under reports_dir\"\"\"\n",
        "        if not os.path.isdir(reports_dir):\n",
        "            raise FileNotFoundError(f\"Directory not found: {reports_dir}\")\n",
        "\n",
        "        files = [f for f in os.listdir(reports_dir) if f.lower().endswith(\".json\")]\n",
        "        print(f\"Found {len(files)} JSON files to index\")\n",
        "\n",
        "        for fname in files:\n",
        "            print(f\"Indexing {fname}...\")\n",
        "            self._index_single_report(os.path.join(reports_dir, fname))\n",
        "\n",
        "    def _index_single_report(self, filepath: str):\n",
        "        \"\"\"Index single report with encoding detection\"\"\"\n",
        "        data = self._read_json_safe(filepath)\n",
        "\n",
        "        paths: List[Dict[str, Any]] = data.get(\"paths\", [])\n",
        "        if not paths:\n",
        "            print(f\"  Warning: No paths found in {filepath}\")\n",
        "            return\n",
        "\n",
        "        documents: List[str] = []\n",
        "        embeddings: List[List[float]] = []\n",
        "        metas: List[Dict[str, Any]] = []\n",
        "        ids: List[str] = []\n",
        "\n",
        "        for i, path in enumerate(paths):\n",
        "            doc = self._format_path_for_indexing(path)\n",
        "            vec = self.embedding_model.encode(doc).tolist()\n",
        "\n",
        "            meta = {\n",
        "                \"file\": os.path.basename(filepath),\n",
        "                \"path_index\": i,\n",
        "                \"startpoint\": path.get(\"startpoint\", {}).get(\"instance\", \"\"),\n",
        "                \"endpoint\": path.get(\"endpoint\", {}).get(\"instance\", \"\"),\n",
        "                \"slack\": path.get(\"summary\", {}).get(\"slack\", None),\n",
        "                \"clock_group\": path.get(\"report\", {}).get(\"group\", \"\"),\n",
        "                \"path_type\": path.get(\"report\", {}).get(\"path_type\", \"\"),\n",
        "            }\n",
        "\n",
        "            documents.append(doc)\n",
        "            embeddings.append(vec)\n",
        "            metas.append(meta)\n",
        "            ids.append(f\"{os.path.basename(filepath)}::{i}\")\n",
        "\n",
        "        self.collection.add(documents=documents, embeddings=embeddings, metadatas=metas, ids=ids)\n",
        "        print(f\"  Indexed {len(paths)} paths from {filepath}\")\n",
        "\n",
        "    def _format_path_for_indexing(self, path: Dict[str, Any]) -> str:\n",
        "        def fmt(v):\n",
        "            return \"N/A\" if v is None else str(v)\n",
        "\n",
        "        header = [\n",
        "            f\"Startpoint: {fmt(path.get('startpoint', {}).get('instance'))}\",\n",
        "            f\"Endpoint: {fmt(path.get('endpoint', {}).get('instance'))}\",\n",
        "            f\"Slack: {fmt(path.get('summary', {}).get('slack'))} ns\",\n",
        "            f\"Arrival: {fmt(path.get('summary', {}).get('data_arrival_time'))} ns\",\n",
        "            f\"Required: {fmt(path.get('summary', {}).get('data_required_time'))} ns\",\n",
        "            f\"Clock Group: {fmt(path.get('report', {}).get('group'))}\",\n",
        "            f\"Path Type: {fmt(path.get('report', {}).get('path_type'))}\",\n",
        "        ]\n",
        "        text = \" | \".join(header)\n",
        "\n",
        "        # Data path details\n",
        "        stages = path.get(\"data_path\", {}).get(\"stages\", [])\n",
        "        if stages:\n",
        "            text += \"\\nData Path:\"\n",
        "            for s in stages:\n",
        "                name = s.get(\"name\", \"N/A\")\n",
        "                cell = s.get(\"cell_type\", \"\")\n",
        "                incr = s.get(\"incr\", None)\n",
        "                text += f\"\\n- {name}\"\n",
        "                if cell:\n",
        "                    text += f\" [{cell}]\"\n",
        "                if incr is not None:\n",
        "                    text += f\" delay={incr}ns\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    def search_timing_data(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
        "        query_vec = self.embedding_model.encode(query).tolist()\n",
        "        result = self.collection.query(query_embeddings=[query_vec], n_results=top_k)\n",
        "        return result\n",
        "\n",
        "    def _get_all_slack_data(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get all slack data from the collection for accurate analysis\"\"\"\n",
        "        all_data = self.collection.get()\n",
        "        all_metas = all_data['metadatas']\n",
        "\n",
        "        all_slack_data = []\n",
        "        for meta in all_metas:\n",
        "            slack = meta.get(\"slack\")\n",
        "            if slack is not None:\n",
        "                try:\n",
        "                    slack_float = float(slack)\n",
        "                    all_slack_data.append({\n",
        "                        \"slack\": slack_float,\n",
        "                        \"startpoint\": meta.get(\"startpoint\", \"\"),\n",
        "                        \"endpoint\": meta.get(\"endpoint\", \"\"),\n",
        "                        \"file\": meta.get(\"file\", \"\"),\n",
        "                        \"path_type\": meta.get(\"path_type\", \"\"),\n",
        "                        \"clock_group\": meta.get(\"clock_group\", \"\")\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        # Sort by slack value (ascending = worst first)\n",
        "        all_slack_data.sort(key=lambda x: x[\"slack\"])\n",
        "        return all_slack_data\n",
        "\n",
        "    def debug_data(self):\n",
        "        \"\"\"Enhanced debug function\"\"\"\n",
        "        print(\"\\n=== DEBUG: Checking indexed data ===\")\n",
        "\n",
        "        all_data = self.collection.get()\n",
        "        print(f\"Total indexed paths: {len(all_data['ids'])}\")\n",
        "\n",
        "        if all_data['metadatas']:\n",
        "            # Get all slack values\n",
        "            all_slacks = []\n",
        "            path_types = set()\n",
        "            clock_groups = set()\n",
        "\n",
        "            for meta in all_data['metadatas']:\n",
        "                slack = meta.get(\"slack\")\n",
        "                if slack is not None:\n",
        "                    try:\n",
        "                        all_slacks.append(float(slack))\n",
        "                    except (ValueError, TypeError):\n",
        "                        pass\n",
        "                path_type = meta.get(\"path_type\", \"\")\n",
        "                if path_type:\n",
        "                    path_types.add(path_type)\n",
        "                clock_group = meta.get(\"clock_group\", \"\")\n",
        "                if clock_group:\n",
        "                    clock_groups.add(clock_group)\n",
        "\n",
        "            if all_slacks:\n",
        "                print(f\"Slack range: {min(all_slacks):.4f} to {max(all_slacks):.4f} ns\")\n",
        "                print(f\"Path types: {', '.join(path_types)}\")\n",
        "                print(f\"Clock groups: {', '.join(clock_groups)}\")\n",
        "\n",
        "                # Show worst 5\n",
        "                sorted_slacks = sorted(all_slacks)\n",
        "                print(f\"Worst 5 slacks: {[f'{s:.4f}' for s in sorted_slacks[:5]]}\")\n",
        "                print(f\"Best 5 slacks: {[f'{s:.4f}' for s in sorted_slacks[-5:]]}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> str:\n",
        "        \"\"\"Improved query method with direct data analysis\"\"\"\n",
        "        print(f\"DEBUG: Searching for: '{question}'\")\n",
        "\n",
        "        # Get ALL slack data for accurate analysis\n",
        "        all_slack_data = self._get_all_slack_data()\n",
        "\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data found.\"\n",
        "\n",
        "        print(f\"DEBUG: Total slack values: {len(all_slack_data)}\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Answer specific questions with direct data analysis\n",
        "        if \"worst slack\" in question_lower or \"minimum slack\" in question_lower:\n",
        "            worst = all_slack_data[0]  # First item after sorting (minimum slack)\n",
        "            return f\"Worst slack: {worst['slack']:.4f} ns\\nPath: {worst['startpoint']} -> {worst['endpoint']}\\nFile: {worst['file']}\\nClock Group: {worst['clock_group']}\"\n",
        "\n",
        "        if \"best slack\" in question_lower or \"highest slack\" in question_lower or \"maximum slack\" in question_lower:\n",
        "            best = all_slack_data[-1]  # Last item after sorting (maximum slack)\n",
        "            return f\"Best slack: {best['slack']:.4f} ns\\nPath: {best['startpoint']} -> {best['endpoint']}\\nFile: {best['file']}\\nClock Group: {best['clock_group']}\"\n",
        "\n",
        "        if \"how many paths\" in question_lower or \"total paths\" in question_lower:\n",
        "            return f\"Total paths in report: {len(all_slack_data)}\"\n",
        "\n",
        "        if \"min paths\" in question_lower or \"max paths\" in question_lower or \"path type\" in question_lower:\n",
        "            path_types = set(item['path_type'] for item in all_slack_data if item['path_type'])\n",
        "            return f\"Path types in report: {', '.join(path_types) if path_types else 'Unknown'}\"\n",
        "\n",
        "        if \"clock group\" in question_lower or \"clock groups\" in question_lower:\n",
        "            clock_groups = set(item['clock_group'] for item in all_slack_data if item['clock_group'])\n",
        "            return f\"Clock groups in report: {', '.join(clock_groups) if clock_groups else 'Unknown'}\"\n",
        "\n",
        "        # NEW: Endpoint frequency analysis\n",
        "        if \"endpoint\" in question_lower and (\"max\" in question_lower or \"most\" in question_lower or \"frequent\" in question_lower):\n",
        "            from collections import Counter\n",
        "            endpoints = [item['endpoint'] for item in all_slack_data if item['endpoint']]\n",
        "            endpoint_counts = Counter(endpoints)\n",
        "            most_common = endpoint_counts.most_common(5)\n",
        "\n",
        "            result = \"Most frequent endpoints:\\n\"\n",
        "            for i, (endpoint, count) in enumerate(most_common):\n",
        "                result += f\"{i+1}. {endpoint}: {count} times\\n\"\n",
        "            return result\n",
        "\n",
        "        # NEW: Startpoint frequency analysis\n",
        "        if \"startpoint\" in question_lower and (\"max\" in question_lower or \"most\" in question_lower or \"frequent\" in question_lower):\n",
        "            from collections import Counter\n",
        "            startpoints = [item['startpoint'] for item in all_slack_data if item['startpoint']]\n",
        "            startpoint_counts = Counter(startpoints)\n",
        "            most_common = startpoint_counts.most_common(5)\n",
        "\n",
        "            result = \"Most frequent startpoints:\\n\"\n",
        "            for i, (startpoint, count) in enumerate(most_common):\n",
        "                result += f\"{i+1}. {startpoint}: {count} times\\n\"\n",
        "            return result\n",
        "\n",
        "        # NEW: Path frequency analysis (startpoint -> endpoint pairs)\n",
        "        if \"path\" in question_lower and (\"max\" in question_lower or \"most\" in question_lower or \"frequent\" in question_lower):\n",
        "            from collections import Counter\n",
        "            paths = [f\"{item['startpoint']} -> {item['endpoint']}\" for item in all_slack_data if item['startpoint'] and item['endpoint']]\n",
        "            path_counts = Counter(paths)\n",
        "            most_common = path_counts.most_common(5)\n",
        "\n",
        "            result = \"Most frequent paths:\\n\"\n",
        "            for i, (path, count) in enumerate(most_common):\n",
        "                result += f\"{i+1}. {path}: {count} times\\n\"\n",
        "            return result\n",
        "\n",
        "        # NEW: Slack distribution analysis\n",
        "        if \"slack range\" in question_lower or \"slack statistics\" in question_lower:\n",
        "            slacks = [item['slack'] for item in all_slack_data]\n",
        "            return f\"\"\"Slack Statistics:\n",
        "- Total paths: {len(all_slack_data)}\n",
        "- Worst slack: {min(slacks):.4f} ns\n",
        "- Best slack: {max(slacks):.4f} ns\n",
        "- Average slack: {sum(slacks)/len(slacks):.4f} ns\n",
        "- Median slack: {sorted(slacks)[len(slacks)//2]:.4f} ns\"\"\"\n",
        "\n",
        "        if \"show paths\" in question_lower or \"list paths\" in question_lower:\n",
        "            # Show first 10 paths\n",
        "            result = f\"First 10 timing paths (out of {len(all_slack_data)}):\\n\"\n",
        "            for i, item in enumerate(all_slack_data[:10]):\n",
        "                result += f\"{i+1}. Slack: {item['slack']:.4f} ns | Path: {item['startpoint']} -> {item['endpoint']}\\n\"\n",
        "            return result\n",
        "\n",
        "        # For other questions, show summary statistics\n",
        "        slacks = [item['slack'] for item in all_slack_data]\n",
        "        path_types = set(item['path_type'] for item in all_slack_data if item['path_type'])\n",
        "        clock_groups = set(item['clock_group'] for item in all_slack_data if item['clock_group'])\n",
        "\n",
        "        return f\"\"\"Timing Report Summary:\n",
        "- Total paths: {len(all_slack_data)}\n",
        "- Worst slack: {min(slacks):.4f} ns\n",
        "- Best slack: {max(slacks):.4f} ns\n",
        "- Average slack: {sum(slacks)/len(slacks):.4f} ns\n",
        "- Path types: {', '.join(path_types) if path_types else 'Unknown'}\n",
        "- Clock groups: {', '.join(clock_groups) if clock_groups else 'Unknown'}\"\"\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Optional: silence HF symlink warning on Windows\n",
        "    # os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
        "\n",
        "    rag = LocalTimingRAG()\n",
        "\n",
        "    # Ensure timing_reports exists and contains JSON files\n",
        "    reports_dir = \"./timing_reports\"\n",
        "    if not os.path.isdir(reports_dir):\n",
        "        print(\"Create ./timing_reports and place your timing JSON files there.\")\n",
        "        return\n",
        "\n",
        "    # Check JSON files first\n",
        "    print(\"=== Verifying JSON files ===\")\n",
        "    for filename in os.listdir(reports_dir):\n",
        "        if filename.endswith('.json'):\n",
        "            rag.verify_timing_json(f\"{reports_dir}/{filename}\")\n",
        "\n",
        "    print(\"\\n=== Indexing timing reports ===\")\n",
        "    rag.index_timing_reports(reports_dir)\n",
        "    print(\"Indexing complete.\")\n",
        "\n",
        "    # DEBUG: Check if data is properly indexed\n",
        "    rag.debug_data()\n",
        "\n",
        "    # Test queries\n",
        "    print(\"\\n=== Testing queries ===\")\n",
        "    # Test these queries to see the new functionality:\n",
        "    test_queries = [\n",
        "    \"which is the endpoint occuring max number of times?\",\n",
        "    \"which is the startpoint occuring max number of times?\",\n",
        "    \"which is the most frequent path?\",\n",
        "    \"show me the most common endpoints\",\n",
        "    \"what are the most frequent startpoints?\",\n",
        "    \"list the most common paths\"\n",
        "]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQ: {query}\")\n",
        "        result = rag.query(query)\n",
        "        print(f\"A: {result}\")\n",
        "\n",
        "    # Interactive REPL\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Timing RAG ready. Type your question (or 'exit').\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            q = input(\"\\nQ> \").strip()\n",
        "        except (EOFError, KeyboardInterrupt):\n",
        "            break\n",
        "        if not q or q.lower() in {\"exit\", \"quit\"}:\n",
        "            break\n",
        "        ans = rag.query(q)\n",
        "        print(f\"\\nA> {ans}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "-pU91fwx2FCn",
        "outputId": "63019829-2cad-4a64-aaba-2c8b7fe7766d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Verifying JSON files ===\n",
            "\n",
            "=== Verifying JSON: ./timing_reports/caravel.min-hkspi_clk-min_timing_full.json ===\n",
            "File: ./timing_reports/caravel.min-hkspi_clk-min_timing_full.json\n",
            "Paths: 659\n",
            "  Path 1:\n",
            "    Startpoint: chip_core/housekeeping/_6778_\n",
            "    Endpoint: chip_core/housekeeping/_6778_\n",
            "    Slack: 0.3252\n",
            "    Clock Group: hkspi_clk\n",
            "  Path 2:\n",
            "    Startpoint: chip_core/housekeeping/_6656_\n",
            "    Endpoint: chip_core/housekeeping/_6654_\n",
            "    Slack: 0.6112\n",
            "    Clock Group: hkspi_clk\n",
            "\n",
            "=== Indexing timing reports ===\n",
            "Found 1 JSON files to index\n",
            "Indexing caravel.min-hkspi_clk-min_timing_full.json...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3321470755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3321470755.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Indexing timing reports ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_timing_reports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Indexing complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3321470755.py\u001b[0m in \u001b[0;36mindex_timing_reports\u001b[0;34m(self, reports_dir)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Indexing {fname}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_single_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_index_single_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3321470755.py\u001b[0m in \u001b[0;36m_index_single_report\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{os.path.basename(filepath)}::{i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Indexed {len(paths)} paths from {filepath}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         self._client._add(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mcollection_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_request\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36m_add\u001b[0;34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    419\u001b[0m         )\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         return self.bindings.add(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import json\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import List, Dict, Any, Optional\n",
        "import shutil\n",
        "\n",
        "class LocalTimingRAG:\n",
        "    def __init__(self, model_name: str = \"microsoft/DialoGPT-large\"):\n",
        "        self.model_name = model_name\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.llm_model, self.tokenizer = self._load_local_llm()\n",
        "\n",
        "        # Get device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Use in-memory database to avoid file permission issues\n",
        "        self._setup_database()\n",
        "\n",
        "        self.history = []\n",
        "\n",
        "    def _setup_database(self):\n",
        "        \"\"\"Setup ChromaDB with in-memory storage\"\"\"\n",
        "        try:\n",
        "            # Use in-memory database to avoid file permission issues\n",
        "            self.client = chromadb.Client()\n",
        "            # Try to get existing collection, if it exists, delete it first\n",
        "            try:\n",
        "                existing_collection = self.client.get_collection(name=\"timing_reports\")\n",
        "                self.client.delete_collection(name=\"timing_reports\")\n",
        "            except:\n",
        "                pass  # Collection doesn't exist, which is fine\n",
        "\n",
        "            self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "            print(\"Using in-memory database\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up database: {e}\")\n",
        "            # Fallback to persistent client with a different path\n",
        "            try:\n",
        "                # Clear existing database\n",
        "                if os.path.exists(\"./temp_chroma_db\"):\n",
        "                    shutil.rmtree(\"./temp_chroma_db\")\n",
        "\n",
        "                self.client = chromadb.PersistentClient(path=\"./temp_chroma_db\")\n",
        "                self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "                print(\"Using persistent database at ./temp_chroma_db\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Error with persistent database: {e2}\")\n",
        "                # Last resort - create a new in-memory client\n",
        "                self.client = chromadb.Client()\n",
        "                try:\n",
        "                    existing_collection = self.client.get_collection(name=\"timing_reports\")\n",
        "                    self.client.delete_collection(name=\"timing_reports\")\n",
        "                except:\n",
        "                    pass\n",
        "                self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "                print(\"Using fallback in-memory database\")\n",
        "\n",
        "    def _load_local_llm(self):\n",
        "        \"\"\"Load the local LLM and tokenizer\"\"\"\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "            )\n",
        "            return model, tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {e}\")\n",
        "            print(\"Falling back to DialoGPT-medium...\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    \"microsoft/DialoGPT-medium\",\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "                )\n",
        "                return model, tokenizer\n",
        "            except Exception as e2:\n",
        "                print(f\"Error loading fallback model: {e2}\")\n",
        "                return None, None\n",
        "\n",
        "    def _read_json_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Read JSON file with proper encoding handling\"\"\"\n",
        "        try:\n",
        "            # Try UTF-8 first\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                # Try UTF-16\n",
        "                with open(file_path, 'r', encoding='utf-16') as f:\n",
        "                    return json.load(f)\n",
        "            except UnicodeDecodeError:\n",
        "                try:\n",
        "                    # Try UTF-16 with BOM\n",
        "                    with open(file_path, 'r', encoding='utf-16-sig') as f:\n",
        "                        return json.load(f)\n",
        "                except UnicodeDecodeError:\n",
        "                    try:\n",
        "                        # Try latin-1 as fallback\n",
        "                        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                            return json.load(f)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading {file_path}: {e}\")\n",
        "                        return {}\n",
        "\n",
        "    def index_timing_reports(self, directory: str):\n",
        "        \"\"\"Index all JSON files in the directory\"\"\"\n",
        "        print(\"=== Indexing timing reports ===\")\n",
        "\n",
        "        json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
        "        print(f\"Found {len(json_files)} JSON files to index\")\n",
        "\n",
        "        for filename in json_files:\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            print(f\"Indexing {filename}...\")\n",
        "\n",
        "            try:\n",
        "                data = self._read_json_file(file_path)\n",
        "                if not data:\n",
        "                    print(f\"Failed to read {filename}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract text and metadata for each path\n",
        "                for i, path in enumerate(data.get('paths', [])):\n",
        "                    # Create a comprehensive text representation\n",
        "                    text_parts = []\n",
        "\n",
        "                    # Add path metadata\n",
        "                    if 'startpoint' in path:\n",
        "                        text_parts.append(f\"Startpoint: {path['startpoint'].get('instance', 'N/A')}\")\n",
        "                    if 'endpoint' in path:\n",
        "                        text_parts.append(f\"Endpoint: {path['endpoint'].get('instance', 'N/A')}\")\n",
        "                    if 'report' in path:\n",
        "                        report = path['report']\n",
        "                        text_parts.append(f\"Group: {report.get('group', 'N/A')}\")\n",
        "                        text_parts.append(f\"Path Type: {report.get('path_type', 'N/A')}\")\n",
        "\n",
        "                    # Add summary information\n",
        "                    if 'summary' in path:\n",
        "                        summary = path['summary']\n",
        "                        if 'slack' in summary:\n",
        "                            text_parts.append(f\"Slack: {summary['slack']}\")\n",
        "                        if 'hold_time_requirement' in summary and summary['hold_time_requirement'] is not None:\n",
        "                            text_parts.append(f\"Hold Time Requirement: {summary['hold_time_requirement']}\")\n",
        "                        if 'clock_skew' in summary and summary['clock_skew'] is not None:\n",
        "                            text_parts.append(f\"Clock Skew: {summary['clock_skew']}\")\n",
        "\n",
        "                    # Add launch clock path stages\n",
        "                    if 'launch_clock_path' in path and 'stages' in path['launch_clock_path']:\n",
        "                        text_parts.append(\"Launch Clock Path:\")\n",
        "                        for stage in path['launch_clock_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Add data path stages\n",
        "                    if 'data_path' in path and 'stages' in path['data_path']:\n",
        "                        text_parts.append(\"Data Path:\")\n",
        "                        for stage in path['data_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Add capture clock path stages\n",
        "                    if 'capture_clock_path' in path and 'stages' in path['capture_clock_path']:\n",
        "                        text_parts.append(\"Capture Clock Path:\")\n",
        "                        for stage in path['capture_clock_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Combine all text\n",
        "                    full_text = \"\\n\".join(text_parts)\n",
        "\n",
        "                    # Generate embedding\n",
        "                    embedding = self.embedding_model.encode(full_text).tolist()\n",
        "\n",
        "                    # Store in ChromaDB\n",
        "                    self.collection.add(\n",
        "                        embeddings=[embedding],\n",
        "                        documents=[full_text],\n",
        "                        metadatas=[{\n",
        "                            'filename': filename,\n",
        "                            'path_index': i,\n",
        "                            'startpoint': path.get('startpoint', {}).get('instance', 'N/A'),\n",
        "                            'endpoint': path.get('endpoint', {}).get('instance', 'N/A'),\n",
        "                            'slack': path.get('summary', {}).get('slack', 'N/A'),\n",
        "                            'hold_time_requirement': path.get('summary', {}).get('hold_time_requirement', 'N/A'),\n",
        "                            'clock_skew': path.get('summary', {}).get('clock_skew', 'N/A'),\n",
        "                            'group': path.get('report', {}).get('group', 'N/A')\n",
        "                        }],\n",
        "                        ids=[f\"{filename}_{i}\"]\n",
        "                    )\n",
        "\n",
        "                print(f\"Successfully indexed {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error indexing {filename}: {e}\")\n",
        "\n",
        "        print(\"Indexing complete!\")\n",
        "\n",
        "    def _get_all_slack_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all slack-related data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('slack') != 'N/A':\n",
        "                try:\n",
        "                    slack_value = float(metadata['slack'])\n",
        "                    all_data.append({\n",
        "                        'slack': slack_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'hold_time_requirement': metadata.get('hold_time_requirement', 'N/A'),\n",
        "                        'clock_skew': metadata.get('clock_skew', 'N/A'),\n",
        "                        'document': results['documents'][i]\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _get_clock_skew_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all clock skew data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('clock_skew') != 'N/A':\n",
        "                try:\n",
        "                    clock_skew_value = float(metadata['clock_skew'])\n",
        "                    all_data.append({\n",
        "                        'clock_skew': clock_skew_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'slack': metadata.get('slack', 'N/A'),\n",
        "                        'document': results['documents'][i]\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _get_hold_time_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all hold time requirement data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('hold_time_requirement') != 'N/A':\n",
        "                try:\n",
        "                    hold_time_value = float(metadata['hold_time_requirement'])\n",
        "                    all_data.append({\n",
        "                        'hold_time_requirement': hold_time_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'slack': metadata.get('slack', 'N/A'),\n",
        "                        'document': results['documents'][i]\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _classify_question(self, question: str, data: List[Dict]) -> str:\n",
        "        \"\"\"Classify the question type using simple pattern matching\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        if \"worst\" in question_lower and \"slack\" in question_lower:\n",
        "            return \"worst_slack\"\n",
        "        elif \"best\" in question_lower and \"slack\" in question_lower:\n",
        "            return \"best_slack\"\n",
        "        elif \"total\" in question_lower or \"number\" in question_lower or \"how many\" in question_lower:\n",
        "            return \"total_paths\"\n",
        "        elif \"type\" in question_lower:\n",
        "            return \"path_types\"\n",
        "        elif \"group\" in question_lower:\n",
        "            return \"clock_groups\"\n",
        "        elif \"stat\" in question_lower:\n",
        "            return \"slack_stats\"\n",
        "        elif \"endpoint\" in question_lower:\n",
        "            return \"endpoint_frequency\"\n",
        "        elif \"hold\" in question_lower and \"worst\" in question_lower:\n",
        "            return \"worst_hold_time_req\"\n",
        "        elif \"hold\" in question_lower and \"best\" in question_lower:\n",
        "            return \"best_hold_time_req\"\n",
        "        elif \"hold\" in question_lower and \"requirement\" in question_lower:\n",
        "            return \"hold_time_req\"\n",
        "        elif \"skew\" in question_lower:\n",
        "            return \"clock_skew\"\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def query(self, question: str, top_k: int = 3) -> str:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        # Add to history\n",
        "        self.history.append({'question': question, 'answer': ''})\n",
        "\n",
        "        # Get all slack data for comprehensive analysis\n",
        "        all_slack_data = self._get_all_slack_data()\n",
        "\n",
        "        # Classify the question\n",
        "        question_type = self._classify_question(question, all_slack_data)\n",
        "\n",
        "        # Generate response based on question type\n",
        "        if question_type == \"worst_slack\":\n",
        "            if not all_slack_data:\n",
        "                return \"No slack data available.\"\n",
        "\n",
        "            worst_slack = min(all_slack_data, key=lambda x: x['slack'])\n",
        "            return f\"The worst slack is {worst_slack['slack']}ns for the path from {worst_slack['startpoint']} to {worst_slack['endpoint']} in group {worst_slack['group']}.\"\n",
        "\n",
        "        elif question_type == \"best_slack\":\n",
        "            if not all_slack_data:\n",
        "                return \"No slack data available.\"\n",
        "\n",
        "            best_slack = max(all_slack_data, key=lambda x: x['slack'])\n",
        "            return f\"The best slack is {best_slack['slack']}ns for the path from {best_slack['startpoint']} to {best_slack['endpoint']} in group {best_slack['group']}.\"\n",
        "\n",
        "        elif question_type == \"total_paths\":\n",
        "            total_paths = len(all_slack_data)\n",
        "            return f\"Total number of timing paths: {total_paths}\"\n",
        "\n",
        "        elif question_type == \"slack_stats\":\n",
        "            if not all_slack_data:\n",
        "                return \"No slack data available.\"\n",
        "\n",
        "            slack_values = [x['slack'] for x in all_slack_data]\n",
        "            avg_slack = sum(slack_values) / len(slack_values)\n",
        "            min_slack = min(slack_values)\n",
        "            max_slack = max(slack_values)\n",
        "\n",
        "            return f\"Slack statistics:\\n- Average: {avg_slack:.3f}ns\\n- Minimum: {min_slack:.3f}ns\\n- Maximum: {max_slack:.3f}ns\\n- Total paths: {len(slack_values)}\"\n",
        "\n",
        "        elif question_type == \"clock_skew\":\n",
        "            clock_skew_data = self._get_clock_skew_data()\n",
        "            if not clock_skew_data:\n",
        "                return \"No clock skew data available.\"\n",
        "\n",
        "            # Find the path with worst slack and get its clock skew\n",
        "            worst_slack_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "            for path in clock_skew_data:\n",
        "                if (path['startpoint'] == worst_slack_path['startpoint'] and\n",
        "                    path['endpoint'] == worst_slack_path['endpoint']):\n",
        "                    return f\"The clock skew for the path from {path['startpoint']} to {path['endpoint']} is {path['clock_skew']}ns.\"\n",
        "\n",
        "            return f\"Clock skew data available for {len(clock_skew_data)} paths. Clock skew values range from {min([x['clock_skew'] for x in clock_skew_data]):.3f}ns to {max([x['clock_skew'] for x in clock_skew_data]):.3f}ns.\"\n",
        "\n",
        "        elif question_type == \"hold_time_req\":\n",
        "            hold_time_data = self._get_hold_time_data()\n",
        "            if not hold_time_data:\n",
        "                return \"No hold time requirement data available.\"\n",
        "\n",
        "            # Find the path with worst slack and get its hold time requirement\n",
        "            worst_slack_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "            for path in hold_time_data:\n",
        "                if (path['startpoint'] == worst_slack_path['startpoint'] and\n",
        "                    path['endpoint'] == worst_slack_path['endpoint']):\n",
        "                    return f\"The hold time requirement for the path from {path['startpoint']} to {path['endpoint']} is {path['hold_time_requirement']}ns.\"\n",
        "\n",
        "            return f\"Hold time requirement data available for {len(hold_time_data)} paths. Hold time requirements range from {min([x['hold_time_requirement'] for x in hold_time_data]):.3f}ns to {max([x['hold_time_requirement'] for x in hold_time_data]):.3f}ns.\"\n",
        "\n",
        "        elif question_type == \"worst_hold_time_req\":\n",
        "            hold_time_data = self._get_hold_time_data()\n",
        "            if not hold_time_data:\n",
        "                return \"No hold time requirement data available.\"\n",
        "\n",
        "            worst_hold_time = min(hold_time_data, key=lambda x: x['hold_time_requirement'])\n",
        "            return f\"The worst hold time requirement is {worst_hold_time['hold_time_requirement']}ns for the path from {worst_hold_time['startpoint']} to {worst_hold_time['endpoint']} in group {worst_hold_time['group']}.\"\n",
        "\n",
        "        elif question_type == \"best_hold_time_req\":\n",
        "            hold_time_data = self._get_hold_time_data()\n",
        "            if not hold_time_data:\n",
        "                return \"No hold time requirement data available.\"\n",
        "\n",
        "            best_hold_time = max(hold_time_data, key=lambda x: x['hold_time_requirement'])\n",
        "            return f\"The best hold time requirement is {best_hold_time['hold_time_requirement']}ns for the path from {best_hold_time['startpoint']} to {best_hold_time['endpoint']} in group {best_hold_time['group']}.\"\n",
        "\n",
        "        else:\n",
        "            # For general questions, provide a simple response without using the LLM\n",
        "            return \"I can help you analyze timing data. Try asking about:\\n- Worst/best slack values\\n- Clock skew analysis\\n- Hold time requirements\\n- Total number of paths\\n- Slack statistics\"\n",
        "\n",
        "def main():\n",
        "    # Create RAG system\n",
        "    rag = LocalTimingRAG()\n",
        "\n",
        "    # Index timing reports\n",
        "    rag.index_timing_reports('./timing_reports/')\n",
        "\n",
        "    # Interactive query loop\n",
        "    print(\"\\n=== Timing RAG System Ready ===\")\n",
        "    print(\"Ask questions about your timing data. Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nQuestion: \").strip()\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if question:\n",
        "            answer = rag.query(question)\n",
        "            print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3BZjI0E1wm0",
        "outputId": "2e8cee30-bce8-4615-aac6-3dba20f06294"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using in-memory database\n",
            "=== Indexing timing reports ===\n",
            "Found 1 JSON files to index\n",
            "Indexing caravel.min-hkspi_clk-min_timing_full.json...\n",
            "Successfully indexed caravel.min-hkspi_clk-min_timing_full.json\n",
            "Indexing complete!\n",
            "\n",
            "=== Timing RAG System Ready ===\n",
            "Ask questions about your timing data. Type 'quit' to exit.\n",
            "\n",
            "Question: what is the worst slack?\n",
            "Answer: The worst slack is 0.3252ns for the path from chip_core/housekeeping/_6778_ to chip_core/housekeeping/_6778_ in group hkspi_clk.\n",
            "\n",
            "Question: what is the clock skew for that slack?\n",
            "Answer: The clock skew for the path from chip_core/housekeeping/_6778_ to chip_core/housekeeping/_6778_ is 0.4998999999999967ns.\n",
            "\n",
            "Question: how much is the hold time requirement for that path?\n",
            "Answer: The hold time requirement for the path from chip_core/housekeeping/_6778_ to chip_core/housekeeping/_6778_ is 0.1002ns.\n",
            "\n",
            "Question: what is the next path slack?\n",
            "Answer: I can help you analyze timing data. Try asking about:\n",
            "- Worst/best slack values\n",
            "- Clock skew analysis\n",
            "- Hold time requirements\n",
            "- Total number of paths\n",
            "- Slack statistics\n",
            "\n",
            "Question: worst slack?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk_q0GeBBq6x"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFn9aEsPxK2H8svRuZs2Nx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}